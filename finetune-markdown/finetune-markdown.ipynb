{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<p>① 회사는 채용이 확정된 자와 서면으로 근로계약을 체결하고, 해당자에게 근로계약서 사본 1부를 내어 준다.\\n② 회사는 근로계약 체결 시 사원에게 임금, 소정근로시간, 휴게시간, 휴일, 연차 유급휴가, 취업의 장소와 종사하여야 할 업무에 관한 사항, 근로기준법 제93조제1호부터 제12호까지의 규정(취업규칙의 작성 ․ 신고사항)에서 정한 사항, 근로기준법 제10장의 기숙사에 관한 사항(기숙사가 있는 경우에 한함)을 명확히 제시한다.\\n③ 회사는 제2항의 내용 중 임금의 구성항목 ․ 계산방법 ․ 지급방법, 소정근로시간, 휴게시간, 휴일, 연차유급휴가에 관한 사항, 취업의 장소와 종사하여야 할 업무에 관한 사항을 서면으로 명확히 제시하여 교부한다. 또한 기간제 근로자인 경우 근로계약기간도 함께 명시한다.\\n④ 회사는 근로계약 체결 시 제2항 및 제3항의 사항이 적시된 취업규칙을 제시하거나 교부함으로써 제2항의 명시 및 제3항의 서면명시 및 교부의무를 대신할 수 있다.</p>']\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import markdown\n",
    "import os\n",
    "\n",
    "# 마크다운 파일 로드\n",
    "with open(\"finetune-markdown.md\", \"r\") as file:\n",
    "    markdown_text = file.read()\n",
    "\n",
    "text = markdown.markdown(markdown_text)\t\t# 마크다운을 일반 텍스트로 변환\n",
    "sections = text.split(\"\\n\\n\")  \t\t\t\t# 헤더로 구분된 섹션을 텍스트를 섹션으로 분할, 마크다운 구조에 따라 분할 기준 조정\n",
    "print(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사전 학습된 임베딩 모델 로드\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# 각 섹션 벡터 임베딩\n",
    "embeddings = model.encode(sections)\n",
    "\n",
    "# 이제 `embeddings`에는 마크다운 문서의 각 섹션에 대한 벡터 표현이 포함됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "\n",
    "def find_relevant_section(question, sections, embeddings):\n",
    "    question_embedding = model.encode([question])\n",
    "    similarities = cosine_similarity(question_embedding, embeddings)\n",
    "    most_relevant_idx = np.argmax(similarities)\n",
    "    return sections[most_relevant_idx]\n",
    "\n",
    "# Load a QA model\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
    "qa_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "def answer_question(question, context, max_length=512):\n",
    "    # 질문과 맥락을 토큰화하고 필요한 경우 잘라냅니다.\n",
    "    inputs = qa_tokenizer.encode_plus(question, context, add_special_tokens=True, max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
    "\n",
    "    # Forward pass through the model\n",
    "    outputs = qa_model(**inputs)\n",
    "    answer_start_scores, answer_end_scores = outputs.start_logits, outputs.end_logits\n",
    "\n",
    "    # '시작' 및 '끝' 점수가 가장 높은 토큰을 찾습니다.\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "\n",
    "    # 토큰을 문자열로 변환\n",
    "    answer = qa_tokenizer.convert_tokens_to_string(qa_tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "# Using the model\n",
    "question = \"What is the main topic?\"\n",
    "relevant_section = find_relevant_section(question, sections, embeddings)\n",
    "print(answer_question(question, relevant_section))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
